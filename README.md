# Sampling
This README file describes various sampling techniques used in machine learning models. This resource aims to help developers and data scientists choose the appropriate sampling method for their specific use case and improve the accuracy of their machine learning models. It provides an overview of the different sampling methods available and their benefits, along with examples of how they can be used in various machine learning applications. The file also includes code snippets and explanations of how to implement the different sampling techniques in popular machine learning libraries


SAMPLING TECHNIQUES USED:
1. Simple Random Sampling - The process of selecting a simple random sample, a subset of individuals chosen from a larger set in which a subset of individuals are chosen randomly, all with the same probability.
2. Stratified Sampling - A type of sampling method in which the total population is divided into smaller groups or strata to complete the sampling process.
3. Cluster Sampling - A sampling plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population.
4. Systematic Sampling - A probability sampling method where researchers select members of the population at a regular interval.
5. Convenience Sampling - A non-probability sampling method where units are selected for inclusion in the sample because they are the easiest for the researcher to access.


MACHINE LEARNING MODELS USED:
1. Random Forest (combines the output of multiple decision trees to reach a single result)
2. Decision Tree (non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks which has a hierarchical, tree structure)
3. KNN (works by finding the K nearest points in the training dataset and uses their class to predict the class or value of a new data point)
4. Logistic Regression (supervised machine learning algorithm that accomplishes binary classification tasks by predicting the probability of an outcome, event, or observation)
5. Support Vector Classifier (t seeks to find the hyperplane that best separates the data points into different classes)
